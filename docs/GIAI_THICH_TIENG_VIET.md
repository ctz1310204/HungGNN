# GI·∫¢I TH√çCH GNN CHO B√ÄI TO√ÅN LINEAR ASSIGNMENT - TI·∫æNG VI·ªÜT

## üéØ B√†i to√°n

**Ph√¢n c√¥ng 4 c√¥ng nh√¢n cho 4 c√¥ng vi·ªác sao cho T·ªîNG CHI PH√ç TH·∫§P NH·∫§T**

V√≠ d·ª•:
```
Cost Matrix:
         CV0    CV1    CV2    CV3
CN0:    0.77   0.56   0.54   0.66
CN1:    0.57   0.66   0.36   0.91
CN2:    0.71   0.91   0.26   0.54
CN3:    0.31   0.52   0.96   0.53

Ph∆∞∆°ng √°n t·ªëi ∆∞u:
- C√¥ng nh√¢n 0 ‚Üí C√¥ng vi·ªác 1 (0.56)
- C√¥ng nh√¢n 1 ‚Üí C√¥ng vi·ªác 2 (0.36)
- C√¥ng nh√¢n 2 ‚Üí C√¥ng vi·ªác 3 (0.54)
- C√¥ng nh√¢n 3 ‚Üí C√¥ng vi·ªác 0 (0.31)
T·ªîNG: 1.78
```

---

## üß† C√°ch GNN ho·∫°t ƒë·ªông

### B∆∞·ªõc 1: Chu·∫©n b·ªã Input (8 nodes √ó 4 features)

```
Input = [Cost Matrix; Cost Matrix Transpose]
      = [4 c√¥ng nh√¢n; 4 c√¥ng vi·ªác]
      = 8 nodes, m·ªói node 4 features
```

**V√≠ d·ª•:**
- Node 0 (CN0): `[0.77, 0.56, 0.54, 0.66]` - chi ph√≠ CN0 cho 4 CV
- Node 4 (CV0): `[0.77, 0.57, 0.71, 0.31]` - chi ph√≠ 4 CN cho CV0

**T·∫°o ƒë·ªì th·ªã bipartite:** M·ªói c√¥ng nh√¢n k·∫øt n·ªëi v·ªõi m·ªçi c√¥ng vi·ªác

---

### B∆∞·ªõc 2: Conv1 - M·ªü r·ªông features (8√ó4 ‚Üí 8√ó32)

```
EdgeConv Layer:
  Input:  8 nodes √ó 4 features
  Output: 8 nodes √ó 32 features
  
C√°ch ho·∫°t ƒë·ªông:
1. Gh√©p n·ªëi features c·ªßa nodes l√°ng gi·ªÅng: [x_i, x_j]
2. ƒê∆∞a qua MLP: Linear(8‚Üí64) ‚Üí ReLU ‚Üí Linear(64‚Üí32)
3. Max aggregation
```

**M·ª•c ƒë√≠ch:** H·ªçc c√°c ƒë·∫∑c tr∆∞ng ·∫©n (hidden features) t·ª´ quan h·ªá gi·ªØa c√¥ng nh√¢n v√† c√¥ng vi·ªác

---

### B∆∞·ªõc 3: Conv2 - Thu h·∫πp v·ªÅ predictions (8√ó32 ‚Üí 8√ó4)

```
EdgeConv Layer:
  Input:  8 nodes √ó 32 features  
  Output: 8 nodes √ó 4 logits
  
C√°ch ho·∫°t ƒë·ªông:
1. Gh√©p n·ªëi features: [x_i, x_j]
2. ƒê∆∞a qua MLP: Linear(64‚Üí64) ‚Üí ReLU ‚Üí Linear(64‚Üí4)
3. Max aggregation
```

**M·ª•c ƒë√≠ch:** T·∫°o d·ª± ƒëo√°n ban ƒë·∫ßu - m·ªói node c√≥ 4 logits (preference scores)

**V√≠ d·ª• Output:**
```
CN0: [1.84, -0.91, 1.03, 0.82]  ‚Üí Th√≠ch CV0 nh·∫•t
CN1: [-0.24, 0.26, -1.90, 2.71] ‚Üí Th√≠ch CV3 nh·∫•t
CN2: [1.96, 3.06, -1.03, 0.69] ‚Üí Th√≠ch CV1 nh·∫•t
CN3: [-1.21, 1.60, 2.85, 1.51] ‚Üí Th√≠ch CV2 nh·∫•t
```

---

### B∆∞·ªõc 4: Readout - T·ªïng h·ª£p cu·ªëi c√πng (8√ó4 ‚Üí 4√ó4)

```
Linear Layer:
  Input:  Transpose(8√ó4) = 4√ó8
  Output: 4√ó4 matrix
  
C√°ch ho·∫°t ƒë·ªông:
  Linear(8 ‚Üí 4) projection
```

**Output cu·ªëi c√πng: Ma tr·∫≠n 4√ó4 logits**

```
         CV0     CV1     CV2     CV3
CN0:   -6.63    6.45   -7.16    2.17   ‚Üê Th√≠ch CV1 (6.45)
CN1:    2.69    1.48    6.32   -2.60   ‚Üê Th√≠ch CV2 (6.32)
CN2:   -4.18   -6.13    4.30    5.69   ‚Üê Th√≠ch CV3 (5.69)
CN3:    5.89   -1.31  -11.47    1.63   ‚Üê Th√≠ch CV0 (5.89)
```

**L∆∞u √Ω:** ƒê√¢y l√† **RAW LOGITS** (kh√¥ng ph·∫£i x√°c su·∫•t)
- C√≥ th·ªÉ √¢m
- KH√îNG t·ªïng = 1
- C√†ng cao = c√†ng th√≠ch

---

### B∆∞·ªõc 5: Softmax - Chuy·ªÉn th√†nh x√°c su·∫•t (optional)

```
√Åp d·ª•ng softmax cho m·ªói h√†ng:
         CV0     CV1     CV2     CV3
CN0:    0.0%   98.6%    0.0%    1.4%   ‚Üê 98.6% ch·∫Øc ch·∫Øn ch·ªçn CV1
CN1:    2.6%    0.8%   96.7%    0.0%   ‚Üê 96.7% ch·∫Øc ch·∫Øn ch·ªçn CV2
CN2:    0.0%    0.0%   20.0%   80.0%   ‚Üê 80.0% ch·ªçn CV3
CN3:   98.5%    0.1%    0.0%    1.4%   ‚Üê 98.5% ch·∫Øc ch·∫Øn ch·ªçn CV0
```

**M·ªói h√†ng t·ªïng = 100%** ‚úÖ

---

### B∆∞·ªõc 6: Greedy Algorithm - Tr√°nh xung ƒë·ªôt

**V·∫•n ƒë·ªÅ:** N·∫øu m·ªói c√¥ng nh√¢n ch·ªâ ch·ªçn c√¥ng vi·ªác y√™u th√≠ch nh·∫•t:
```
CN0 ‚Üí CV1
CN1 ‚Üí CV2  
CN2 ‚Üí CV3
CN3 ‚Üí CV0
```

C√≥ th·ªÉ b·ªã **TR√ôNG** (nhi·ªÅu ng∆∞·ªùi ch·ªçn c√πng 1 c√¥ng vi·ªác)!

**Gi·∫£i ph√°p - Greedy Algorithm:**

```
1. T√¨m gi√° tr·ªã MAX trong to√†n b·ªô ma tr·∫≠n
2. Ph√¢n c√¥ng: c√¥ng nh√¢n ƒë√≥ ‚Üí c√¥ng vi·ªác ƒë√≥
3. MASK (lo·∫°i b·ªè) h√†ng v√† c·ªôt ƒë√£ ch·ªçn
4. L·∫∑p l·∫°i cho ƒë·∫øn h·∫øt
```

**V√≠ d·ª•:**
```
Step 1: Max = 6.45 t·∫°i (CN0, CV1)
  ‚Üí Ph√¢n c√¥ng: CN0 ‚Üí CV1
  ‚Üí Mask h√†ng 0 v√† c·ªôt 1
  
Step 2: Max = 6.32 t·∫°i (CN1, CV2)
  ‚Üí Ph√¢n c√¥ng: CN1 ‚Üí CV2
  ‚Üí Mask h√†ng 1 v√† c·ªôt 2
  
Step 3: Max = 5.89 t·∫°i (CN3, CV0)
  ‚Üí Ph√¢n c√¥ng: CN3 ‚Üí CV0
  ‚Üí Mask h√†ng 3 v√† c·ªôt 0
  
Step 4: Max = 5.69 t·∫°i (CN2, CV3)
  ‚Üí Ph√¢n c√¥ng: CN2 ‚Üí CV3
  ‚Üí Mask h√†ng 2 v√† c·ªôt 3
```

**K·∫øt qu·∫£:** `[1, 2, 3, 0]` - M·ªói c√¥ng nh√¢n 1 c√¥ng vi·ªác, kh√¥ng tr√πng! ‚úÖ

---

## üìä Gi·∫£i th√≠ch c√°c CH·ªà S·ªê TH·ªêNG K√ä

### 1. Shape (K√≠ch th∆∞·ªõc)
```
Shape: (8, 4) = 8 h√†ng, 4 c·ªôt
Shape: (4, 32) = 4 h√†ng, 32 c·ªôt
```

### 2. Min (Gi√° tr·ªã nh·ªè nh·∫•t)
```
Min: -11.47
‚Üí S·ªë nh·ªè nh·∫•t trong ma tr·∫≠n
‚Üí Cho bi·∫øt c·∫≠n d∆∞·ªõi c·ªßa d·ªØ li·ªáu
```

### 3. Max (Gi√° tr·ªã l·ªõn nh·∫•t)
```
Max: 6.45
‚Üí S·ªë l·ªõn nh·∫•t trong ma tr·∫≠n
‚Üí Logit cao = ƒë∆∞·ª£c ∆∞u ti√™n ch·ªçn
```

### 4. Mean (Trung b√¨nh)
```
Mean: 0.61
C√¥ng th·ª©c: T·ªïng t·∫•t c·∫£ / s·ªë ph·∫ßn t·ª≠
‚Üí Gi√° tr·ªã trung t√¢m
‚Üí Cho bi·∫øt d·ªØ li·ªáu nghi√™ng v·ªÅ ƒë√¢u
```

### 5. Std (ƒê·ªô l·ªách chu·∫©n) ‚≠ê QUAN TR·ªåNG

**ƒêo ƒë·ªô PH√ÇN T√ÅN c·ªßa d·ªØ li·ªáu:**

```
Std CAO (>5):
  ‚Üí D·ªØ li·ªáu r·∫£i r·ªông, kh√°c bi·ªát l·ªõn
  ‚Üí Model T·ª∞ TIN, c√≥ l·ª±a ch·ªçn R√ï R√ÄNG
  
Std TH·∫§P (<2):
  ‚Üí D·ªØ li·ªáu t·∫≠p trung, g·∫ßn b·∫±ng nhau
  ‚Üí Model KH√îNG CH·∫ÆC CH·∫ÆN, kh√≥ quy·∫øt ƒë·ªãnh
```

**V√≠ d·ª• d·ªÖ hi·ªÉu:**

```
C√¥ng nh√¢n A ƒë√°nh gi√° 4 c√¥ng vi·ªác:
[2, 9, 3, 2]
  ‚Üí R√µ r√†ng TH√çCH c√¥ng vi·ªác 2 (9 ƒëi·ªÉm) nh·∫•t!
  ‚Üí Std = 3.5 (CAO)
  ‚Üí Model T·ª∞ TIN ‚úÖ

C√¥ng nh√¢n B ƒë√°nh gi√° 4 c√¥ng vi·ªác:
[5, 6, 5, 5]
  ‚Üí Kh√¥ng r√µ th√≠ch c√°i n√†o, g·∫ßn b·∫±ng nhau
  ‚Üí Std = 0.5 (TH·∫§P)
  ‚Üí Model KH√îNG CH·∫ÆC ‚ö†Ô∏è
```

---

## ‚ö†Ô∏è Khi n√†o Greedy TH·∫§T B·∫†I?

**T√¨nh hu·ªëng:** Sau 3 b∆∞·ªõc greedy, t·∫•t c·∫£ gi√° tr·ªã c√≤n l·∫°i **B·∫∞NG NHAU**

```
V√≠ d·ª• Sample #214:

Initial logits: C√≥ variance t·ªët ‚úì
[-10.67  -1.60   0.02  -2.63]
[  2.04   2.29  -7.51  -2.16]
[  1.71  -9.20  -3.39   2.63]
[-11.53   1.74  -0.18   0.33]

Step 1: Pick (2,3) ‚úì
Step 2: Pick (1,1) ‚úì  
Step 3: Pick (0,2) ‚úì

Step 4: ALL remaining values = -11.53 ‚ùå
  ‚Üí To√†n b·ªô c√≤n l·∫°i uniform!
  ‚Üí np.where() returns ALL 16 positions
  ‚Üí pp matrix becomes all 1s
  ‚Üí argmax returns [0,0,0,0] (INVALID!)
```

**T·ª∑ l·ªá th·∫•t b·∫°i:** 75/20,000 = 0.38%

---

## üéØ T·ªïng k·∫øt

### Lu·ªìng x·ª≠ l√Ω
```
Cost Matrix (4√ó4)
    ‚Üì
Input Prep (8√ó4)     ‚Üê Bi·∫øn th√†nh graph
    ‚Üì
Conv1 (8√ó32)         ‚Üê M·ªü r·ªông features
    ‚Üì
Conv2 (8√ó4)          ‚Üê Thu h·∫πp th√†nh logits
    ‚Üì
Readout (4√ó4)        ‚Üê T·ªïng h·ª£p cu·ªëi c√πng
    ‚Üì
Softmax (4√ó4)        ‚Üê Chuy·ªÉn th√†nh x√°c su·∫•t (optional)
    ‚Üì
Greedy (4,)          ‚Üê Tr√°nh xung ƒë·ªôt
    ‚Üì
Assignment [1,2,3,0] ‚Üê K·∫øt qu·∫£ cu·ªëi c√πng
```

### K·∫øt qu·∫£
- ‚úÖ **92.02% full row accuracy** - T√¨m ƒë√∫ng ph∆∞∆°ng √°n t·ªëi ∆∞u
- ‚úÖ **99.62% validity** - Greedy t·∫°o ph√¢n c√¥ng h·ª£p l·ªá
- ‚ö†Ô∏è **0.38% failure** - Greedy th·∫•t b·∫°i khi c√≤n l·∫°i uniform

### ∆Øu ƒëi·ªÉm
- Nhanh h∆°n Hungarian Algorithm (~180x)
- H·ªçc ƒë∆∞·ª£c patterns t·ª´ d·ªØ li·ªáu
- Kh·∫£ nƒÉng generalize t·ªët

### Nh∆∞·ª£c ƒëi·ªÉm
- Kh√¥ng ƒë·∫£m b·∫£o 100% t·ªëi ∆∞u
- Greedy c√≥ th·ªÉ fail trong rare cases
- Ph·ª• thu·ªôc v√†o quality c·ªßa training data

---

## üí° Key Takeaways

1. **GNN kh√¥ng output x√°c su·∫•t tr·ª±c ti·∫øp**, m√† output **raw logits**
2. **CrossEntropyLoss** t·ª± ƒë·ªông apply softmax trong qu√° tr√¨nh training
3. **Greedy algorithm** ho·∫°t ƒë·ªông tr√™n raw logits (kh√¥ng ph·∫£i probabilities)
4. **Std cao** = model t·ª± tin, c√≥ s·ª± l·ª±a ch·ªçn r√µ r√†ng
5. **Std th·∫•p** = model kh√¥ng ch·∫Øc ch·∫Øn, c√°c gi√° tr·ªã g·∫ßn nhau
6. Greedy th·∫•t b·∫°i khi remaining values uniform sau masking
